{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of seq2seq_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshil-p02/NLP-chess/blob/master/Copy_of_seq2seq_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHYw_8pQ-KYT",
        "outputId": "8f5e1e54-ada6-4ee7-b551-672681e6b245"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvd_PrCj-ReV",
        "outputId": "e3766882-82ed-442a-c1db-ea1db4439328"
      },
      "source": [
        "!pip install chess"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chess\n",
            "  Downloading chess-1.7.0-py3-none-any.whl (147 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▎                             | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 51 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 61 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 71 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 81 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 92 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 102 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 112 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 122 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 133 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 143 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 147 kB 12.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mItwLxh-RYL",
        "outputId": "43f55164-fa57-4704-afff-e52a074763a5"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP-Chess_lang-model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-Chess_lang-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwAh-g2N-RNV"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-T3rMAT-RJe"
      },
      "source": [
        "!chmod +x /content/drive/MyDrive/NLP-Chess_lang-model/stockfish_13_linux_x64_avx2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5UVeEjF-bYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVU0bxl9AW2s"
      },
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Di_jGnL8en"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgIo2bevL8ep"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCiew_W6uwR3"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "\n",
        "SOS_WORD = \"<sos>\"\n",
        "EOS_WORD = \"<eos>\"\n",
        "\n",
        "SRC = Field(init_token=SOS_WORD, eos_token=EOS_WORD)\n",
        "TRG = Field(init_token=SOS_WORD, eos_token=EOS_WORD)\n",
        "\n",
        "train_data, val_data, test_data = data.TabularDataset.splits(path='/content/drive/MyDrive/NLP-Chess_lang-model/8',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t train=\"train-split.csv\", validation=\"validate-split.csv\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t test=\"test-split.csv\", format=\"csv\", fields=[(\"src\", SRC),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(\"trg\", TRG)])\n",
        "# SRC.build_vocab(train_data.src)\n",
        "# TRG.build_vocab(train_data.trg)\n",
        "SRC.build_vocab(train_data.src, train_data.trg, val_data.src, val_data.trg, test_data.src, test_data.trg)\n",
        "TRG.build_vocab(train_data.src, train_data.trg, val_data.src, val_data.trg, test_data.src, test_data.trg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBX9qHe3fZS8"
      },
      "source": [
        "print(SRC.vocab.itos[00:30])\n",
        "print(TRG.vocab.itos[10:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtkpIAzJL8ez",
        "outputId": "72b8d451-dd6d-4756-af19-772384a66d04"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (de) vocabulary: 97\n",
            "Unique tokens in target (en) vocabulary: 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7DtdQLydFMu",
        "outputId": "c88069f8-6d8a-4cf6-c1e4-ebc7bd0555dc"
      },
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Group similar length text sequences together in batches.\n",
        "train_dataloader, valid_dataloader,test_dataloader = data.BucketIterator.splits(\n",
        "\n",
        "                              # Datasets for iterator to draw data from\n",
        "                              (train_data, val_data, test_data),\n",
        "\n",
        "                              batch_size = batch_size,\n",
        "                              sort = False,\n",
        "                              device = device\n",
        "                              )\n",
        "\n",
        "# Print number of batches in each split.\n",
        "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
        "print('Created `valid_dataloader` with %d batches!'%len(valid_dataloader))\n",
        "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))\n",
        "print(f'batchsize = {batch_size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created `train_dataloader` with 2276 batches!\n",
            "Created `valid_dataloader` with 127 batches!\n",
            "Created `test_dataloader` with 127 batches!\n",
            "batchsize = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X81NJi5L8e0"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxGzAE6L8e1"
      },
      "source": [
        "class Move_Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKHeIr64JMAW"
      },
      "source": [
        "class Commentary_Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcBn6A2wL8e2"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, move_decoder, commentary_decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.move_decoder = move_decoder\n",
        "        self.commentary_decoder = commentary_decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == move_decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.hid_dim == commentary_decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == move_decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        assert encoder.n_layers == commentary_decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        t = 1\n",
        "        #insert input token embedding, previous hidden and previous cell states\n",
        "        #receive output tensor (predictions) and new hidden and cell states\n",
        "        output, hidden, cell = self.move_decoder(input, hidden, cell)\n",
        "         \n",
        "        #place predictions in a tensor holding predictions for each token\n",
        "        outputs[t] = output\n",
        "         \n",
        "        #decide if we are going to use teacher forcing or not\n",
        "        teacher_force = random.random() < teacher_forcing_ratio\n",
        "         \n",
        "        #get the highest predicted token from our predictions\n",
        "        top1 = output.argmax(1) \n",
        "         \n",
        "        #if teacher forcing, use actual next token as next input\n",
        "        #if not, use predicted token\n",
        "        input = trg[t] if teacher_force else top1\n",
        "        t += 1\n",
        "\n",
        "        # till move ending token\n",
        "        while t < trg_len and trg[t-1] != \"]\":\n",
        "          #insert input token embedding, previous hidden and previous cell states\n",
        "          #receive output tensor (predictions) and new hidden and cell states\n",
        "          output, hidden, cell = self.move_decoder(input, hidden, cell)\n",
        "          \n",
        "          #place predictions in a tensor holding predictions for each token\n",
        "          outputs[t] = output\n",
        "          \n",
        "          #decide if we are going to use teacher forcing or not\n",
        "          teacher_force = random.random() < teacher_forcing_ratio\n",
        "          \n",
        "          #get the highest predicted token from our predictions\n",
        "          top1 = output.argmax(1) \n",
        "          \n",
        "          #if teacher forcing, use actual next token as next input\n",
        "          #if not, use predicted token\n",
        "          input = trg[t] if teacher_force else top1\n",
        "          t += 1\n",
        "\n",
        "        # now generate commentary\n",
        "        while t < trg_len:\n",
        "          output, hidden, cell = self.commentary_decoder(input, hidden, cell)\n",
        "          outputs[t] = output\n",
        "          teacher_force = random.random() < teacher_forcing_ratio\n",
        "          top1 = output.argmax(1) \n",
        "          input = trg[t] if teacher_force else top1\n",
        "          t += 1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2GHTHV4L8e2",
        "outputId": "d1b65c2d-7fe0-46be-a2a2-fab85de1fa38"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 256\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec1 = Move_Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "dec2 = Commentary_Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "# device = 'cpu'\n",
        "model = Seq2Seq(enc, dec1, dec2, device).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZsG75G4L8e3",
        "outputId": "28801aa9-02e5-451a-86c6-d47df20c3e9f"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(100, 256)\n",
              "    (rnn): LSTM(256, 256, dropout=0.2)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (move_decoder): Move_Decoder(\n",
              "    (embedding): Embedding(101, 256)\n",
              "    (rnn): LSTM(256, 256, dropout=0.2)\n",
              "    (fc_out): Linear(in_features=256, out_features=101, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (commentary_decoder): Commentary_Decoder(\n",
              "    (embedding): Embedding(101, 256)\n",
              "    (rnn): LSTM(256, 256, dropout=0.2)\n",
              "    (fc_out): Linear(in_features=256, out_features=101, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiqcleIdL8e3",
        "outputId": "a4ffdcd1-952d-4757-d94e-ff1c5733f6c8"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,708,234 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuNqGKxRL8e3"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpILRf4AL8e4"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cljNrlI6L8e4"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyYnGyGzL8e5"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            \n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKpKrP4eL8e5"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5bxxX2SL8e5"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'seperated_move.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93y0z_RIL8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7986199e-b83d-4c38-e160-1040d922b9d5"
      },
      "source": [
        "model.load_state_dict(torch.load('seperated_move.pt'))\n",
        "\n",
        "test_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 1.381 | Test PPL:   3.979 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GafVgWqwi6Fe"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    tokens = sentence\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            output, hidden, _ = model.move_decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # stopping prediction at ']' ---> move ending\n",
        "        if pred_token == trg_field.vocab.stoi[']']:\n",
        "            break\n",
        "\n",
        "    for i in range(max_len):\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, _ = model.commentary_decoder(trg_tensor, hidden, encoder_outputs)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # stopping prediction at ']' ---> move ending\n",
        "        if pred_token == trg_field.vocab.stoi['<eos>']:\n",
        "            break\n",
        "      \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fammFRp1qpxu",
        "outputId": "ab075203-9efd-4cf9-c314-a8314aced613"
      },
      "source": [
        "import asyncio\n",
        "import copy\n",
        "\n",
        "import chess.engine\n",
        "import chess.pgn\n",
        "import chess\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# e4 c5 2. c3 Nc6 3. d4 cxd4 4. cxd4 e6 5. Nf3 Bb4+ 6. Nc3 d6 7. Bd2 Nge7 8. d5 exd5\n",
        "# src = ['e4', 'c5', 'c3', 'Nc6', 'd4', 'cxd4', 'cxd4', 'e6', 'Nf3', 'Bb4+', 'Nc3', 'd6', 'Bd2', 'Nge7', 'd5']\n",
        "# trg = ['Rad8']\n",
        "# res = ['exd5', '<eos>']\n",
        "\n",
        "\n",
        "def stockfish14_scores(move_list, prediction):\n",
        "\n",
        "    score = []\n",
        "    board_in = chess.Board()\n",
        "    for move in move_list:\n",
        "      # illegal move in inital game seq\n",
        "      try:\n",
        "        board_in.push_san(move)\n",
        "      except ValueError:\n",
        "        return score\n",
        "    # use the input board with initial game state\n",
        "\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(\"/content/drive/MyDrive/NLP-Chess_lang-model/stockfish_13_linux_x64_avx2\")\n",
        "    # Scoring ----> Mate(-0) < Mate(-1) < Cp(-50) < Cp(200) < Mate(4) < Mate(1) < MateGiven\n",
        "\n",
        "    # using copy to have two diff boards for evaluation\n",
        "    eval_board = copy.deepcopy(board_in)\n",
        "\n",
        "    info = engine.analyse(eval_board, chess.engine.Limit(time=0.1))\n",
        "    score.append(info[\"score\"].white().wdl().expectation())\n",
        "    # win-draw-lose percentage from white's perspective \n",
        "    try:\n",
        "        board_in.push_san(prediction[0])\n",
        "    except ValueError:\n",
        "        # print(\"Illegal move\")\n",
        "        score.append(float('-inf'))\n",
        "    else:\n",
        "        info = engine.analyse(board_in, chess.engine.Limit(time=0.1))\n",
        "        score.append(info['score'].white().wdl().expectation())\n",
        "\n",
        "    result = engine.play(eval_board, chess.engine.Limit(time=0.1))\n",
        "    eval_board.push(result.move)\n",
        "    info = engine.analyse(eval_board, chess.engine.Limit(time=0.1))\n",
        "    score.append(info['score'].white().wdl().expectation())\n",
        "\n",
        "    engine.quit()\n",
        "    return score\n",
        "\n",
        "# pred = ['err']\n",
        "# score = stockfish14_scores(src, res)\n",
        "# +ve score --------> stockfish better\n",
        "# if score == inf  ------> illelagl move predicted\n",
        "# (score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NH5Jnta-3Q2",
        "outputId": "7a6c7996-6044-4e45-9c0a-0cbe26b9f006"
      },
      "source": [
        "len(test_data.examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8089"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDnaAf7Vi6ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9615b73f-b4c6-4d64-c79d-835857b63480"
      },
      "source": [
        "# Stockfish Evaluation\n",
        "from tqdm import tqdm\n",
        "\n",
        "stockfish_evaluations = []\n",
        "for example in tqdm(test_data.examples):  \n",
        "  move_list = []\n",
        "  input_ = example.src\n",
        "\n",
        "  for j in range(len(input_)):\n",
        "    if input_[j] == '[':\n",
        "      move = ''\n",
        "      while input_[j+1] != ']':\n",
        "        move += input_[j+1]\n",
        "        j += 1\n",
        "      move_list.append(move)\n",
        "  # print(move_list)\n",
        "\n",
        "  target = example.trg\n",
        "  \n",
        "  # print(f'src = {input_}')\n",
        "  # print(f'trg = {target}')\n",
        "  \n",
        "  pred_out = (translate_sentence(input_, SRC, TRG, model, device))\n",
        "  # print(pred_out)\n",
        "  pred = ['err']\n",
        "  for i in range(len(pred_out)):\n",
        "    if pred_out[i] == '[':\n",
        "      pred = pred_out[i+1:-1]\n",
        "\n",
        "  # print(pred)\n",
        "  prediction = [''.join(pred)]\n",
        "  # print(prediction)\n",
        "  stockfish_evaluations.append(stockfish14_scores(move_list, prediction))\n",
        "print(stockfish_evaluations)\n",
        "# train with whole moves\n",
        "# train with harshil's input method with 2 decoders. one for piece and one for co-ords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 1506/8089 [28:35<2:05:05,  1.14s/it]/usr/local/lib/python3.7/dist-packages/chess/__init__.py:3550: RuntimeWarning: coroutine 'stockfish14_scores.<locals>.main' was never awaited\n",
            "  self._attacked_for_king(king_path | king, self.occupied ^ king) or\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            " 93%|█████████▎| 7507/8089 [2:22:38<11:03,  1.14s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxFc-vz4BDcz"
      },
      "source": [
        "base = \"/content/drive/MyDrive/NLP-Chess_lang-model\"\n",
        "with open(base + \"/\" + \"scores.txt\", 'w') as f:\n",
        "  for score in tqdm(stockfish_evaluations):\n",
        "    f.write(score)\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MB1oDVN-2S8"
      },
      "source": [
        "  # below is the code for creating stockfish games\n",
        "  # IGNORE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wB06ldc-bVt"
      },
      "source": [
        "import asyncio\n",
        "import chess\n",
        "import chess.engine\n",
        "import chess.pgn as pgn\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "base = \"/content/drive/MyDrive/NLP-Chess_lang-model\"\n",
        "base1 = \"/content/drive/MyDrive/NLP-Chess_lang-model/stockfish_moves_\"\n",
        "\n",
        "pgn_data = open(\"/content/drive/MyDrive/NLP-Chess_lang-model/Caissabase_PGN.pgn\")\n",
        "boards = pgn.read_game(pgn_data)\n",
        "\n",
        "\n",
        "def stockfish_player(file):\n",
        "    game_list = []\n",
        "    file = str(file)\n",
        "    global boards\n",
        "    games = open(base + \"/\" + file + \"_caissabase_data.txt\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    t1 = t0\n",
        "    t2 = t0\n",
        "    for i in tqdm(range(80000)):\n",
        "        # read a game\n",
        "        game = games.readline().split()\n",
        "        board = boards.board()\n",
        "        # white moves first.\n",
        "        stockfish_moves_list = []\n",
        "        original_moves = game\n",
        "\n",
        "        # if i < 45000:\n",
        "        #    continue\n",
        "\n",
        "        # take the game till mid point\n",
        "        for j in range(len(original_moves)//2):\n",
        "            board.push(chess.Move.from_uci(original_moves[j]))\n",
        "            stockfish_moves_list.append(str(original_moves[j]))\n",
        "        # use stockfish\n",
        "\n",
        "        async def main() -> None:\n",
        "            transport, engine = await chess.engine.popen_uci(\"/content/drive/MyDrive/NLP-Chess_lang-model/stockfish_13_linux_x64_avx2\")\n",
        "            while not board.is_game_over():\n",
        "                result = await engine.play(board, chess.engine.Limit(time=0.1, depth=18))\n",
        "                board.push(result.move)\n",
        "                stockfish_moves_list.append(str(result.move))\n",
        "\n",
        "            await engine.quit()\n",
        "        asyncio.set_event_loop_policy(chess.engine.EventLoopPolicy())\n",
        "        asyncio.run(main())\n",
        "        game_list.append(stockfish_moves_list)\n",
        "        # convt move list to string and write in a file\n",
        "\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            with open(base1 + file + \"_\" + \"0\" + \"-\" + str(i+1) + \".txt\", \"w\") as f:\n",
        "                for move_list in game_list:\n",
        "                    str1 = \" \"\n",
        "                    str1 = str1.join(move_list)\n",
        "                    f.write(str1)\n",
        "                    f.write(\"\\n\\n\")\n",
        "            t2 = time.time()\n",
        "            print(f\"Process {file}: 0-{i+1} games in {t2-t1}\")\n",
        "            t1 = t2\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    NUM_PROCESSES = 2\n",
        "    offset = 8\n",
        "\n",
        "    jobs = []\n",
        "    for i in range(NUM_PROCESSES):\n",
        "        jobs.append(i+offset)\n",
        "    print(jobs)\n",
        "\n",
        "    pool = mp.Pool(NUM_PROCESSES).map(stockfish_player, jobs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}